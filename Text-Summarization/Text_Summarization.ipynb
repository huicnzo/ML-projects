{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Summarization",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RsVA2yRYY4z",
        "outputId": "6fd76eef-7134-4e8f-fec2-5f2aa0a79eb7"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "import string\r\n",
        "from heapq import nlargest"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26Q4IglaYdQK",
        "outputId": "c9736818-e3b0-446e-945c-febde6c926aa"
      },
      "source": [
        "text = \"enter your desired text\"\r\n",
        "if text.count(\". \") > 20:\r\n",
        "    length = int(round(text.count(\". \")/10, 0))\r\n",
        "else:\r\n",
        "    length = 1\r\n",
        "\r\n",
        "nopuch =[char for char in text if char not in string.punctuation]\r\n",
        "nopuch = \"\".join(nopuch)\r\n",
        "\r\n",
        "processed_text = [word for word in nopuch.split() if word.lower() not in nltk.corpus.stopwords.words('english')]\r\n",
        "\r\n",
        "word_freq = {}\r\n",
        "for word in processed_text:\r\n",
        "    if word not in word_freq:\r\n",
        "        word_freq[word] = 1\r\n",
        "    else:\r\n",
        "        word_freq[word] = word_freq[word] + 1\r\n",
        "\r\n",
        "max_freq = max(word_freq.values())\r\n",
        "for word in word_freq.keys():\r\n",
        "    word_freq[word] = (word_freq[word]/max_freq)\r\n",
        "\r\n",
        "sent_list = nltk.sent_tokenize(text)\r\n",
        "sent_score = {}\r\n",
        "for sent in sent_list:\r\n",
        "    for word in nltk.word_tokenize(sent.lower()):\r\n",
        "        if word in word_freq.keys():\r\n",
        "            if sent not in sent_score.keys():\r\n",
        "                sent_score[sent] = word_freq[word]\r\n",
        "            else:\r\n",
        "                sent_score[sent] = sent_score[sent] + word_freq[word]\r\n",
        "\r\n",
        "summary_sents = nlargest(length, sent_score, key=sent_score.get)\r\n",
        "summary = \" \".join(summary_sents)\r\n",
        "print(summary)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPBGJGwkYiC9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}